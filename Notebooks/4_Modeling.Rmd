---
title: "Random_Forest"
output: pdf_document
date: "2025-06-13"
---

# Modeling 

```{r}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
library(caret)
library(here)
library(pROC)
library(pls)
```

## Read in datasets
```{r}
df_model <- readRDS(here("Data/Clean", "heart_disease_modeling.rds"))
# doesn't have correalted feaures ^^
df_final <- readRDS(here("Data/Clean", "heart_disease_final.rds"))
#df_final has correlated features ^^

seed <- 13579
set.seed(seed)
head(df_model)
```
## Ensure positive class and balance
```{r}
# ensure positive class
df_model$has_disease <- factor(df_model$has_disease, levels = c("No", "Yes"))

# check for class balance
table(df_model$has_disease)
prop.table(table(df_model$has_disease))
```

## Global Parameter Setting
```{r control_tuning_grids}
# set global performance metric
global_metric = "ROC"

#resampling strategy
ctrl <- trainControl(
  method = "cv", 
  number = 5, 
  classProbs = TRUE, 
  summaryFunction = twoClassSummary, 
  verboseIter = FALSE
)

# Pre-processing Strategy
process_strat <- NULL


# Penalized Regression Grids

# KNN Grids
K_grid <- data.frame(
  k = seq(1, 25, by = 2)
)

# PLS Tuning Grid for number of components to retain
pls_grid <- expand.grid(.ncomp = 1:10)

# Results Table Creation
# insert darrens code from Performance here and alter variables 




```

## Train/Test Split
```{r}
train_index <- createDataPartition(df_model$has_disease, p = 0.8, list = FALSE)

train_data <- df_model[train_index, ]
test_data <- df_model[-train_index, ]

```

# Logistic Regression

```{r}
glm_model <- train(
  has_disease ~ ., 
  data = train_data,
  method = "glm", 
  family = "binomial",
  metric = "ROC",
  trControl = ctrl
)

# Class predictions
glm_preds <- predict(glm_model, test_data)

# Confusion matrix
confusionMatrix(glm_preds, test_data$has_disease, positive = "Yes")

```

```{r}
glm_probs <- predict(glm_model, test_data, type = "prob")[, "Yes"]
roc_glm <- roc(test_data$has_disease, glm_probs)

# AUC
auc(roc_glm)

# Plot
plot(roc_glm, main = "ROC Curve - Logistic Regression", col = "blue")

```

# Random Forest

```{r}
# create rf model
rfm_model <- train(
  has_disease ~ ., 
  data = train_data, 
  method = "rf", 
  trControl = ctrl, 
  metric = "ROC", 
  importance = TRUE
)

levels(test_data$has_disease)

rfm_preds <- predict(rfm_model, test_data)
confusionMatrix(rfm_preds, test_data$has_disease, positive = "Yes")

rfm_probs <- predict(rfm_model, test_data, type = "prob")

varImp(rfm_model) %>%
  plot()
```

```{r}
# Predict on training data
train_preds <- predict(rfm_model, train_data)

# Predict on test data
test_preds <- predict(rfm_model, test_data)
```

```{r}
# Training set performance
train_cm <- confusionMatrix(train_preds, train_data$has_disease, positive = "Yes")
print(train_cm)

# Test set performance
test_cm <- confusionMatrix(test_preds, test_data$has_disease, positive = "Yes")
print(test_cm)

```

```{r}
# Class probabilities
train_probs <- predict(rfm_model, train_data, type = "prob")[, "Yes"]
test_probs  <- predict(rfm_model, test_data, type = "prob")[, "Yes"]

# ROC curves
roc_train <- roc(train_data$has_disease, train_probs)
roc_test  <- roc(test_data$has_disease, test_probs)

# Print AUC
cat("Train AUC:", auc(roc_train), "\n")
cat("Test AUC:", auc(roc_test), "\n")

# Optional: plot both curves
plot(roc_train, col = "blue", main = "ROC Curves: Train vs Test")
lines(roc_test, col = "red")
legend("bottomright", legend = c("Train", "Test"), col = c("blue", "red"), lwd = 2)

```
```{r}
cat("Train AUC:", auc(roc_train), "\n")
cat("Test AUC:", auc(roc_test), "\n")

```

# Penalized Classification (Darren)
```{r}

```

# Linear Discriminant Analysis (Darren)

```{r}
lda_mod <- train(num ~ ., data = train,
                 method = "lda",
                 metric = "ROC",
                 trControl = trainControl(method = "cv", number = 5))

lda_mod
```

These models can be compared against each other so keep them next to one another

# Partial Least Squares Discriminant Analysis (Graham)

```{r}
# Fit PLSDA model
pls_model <- train(
  has_disease ~ .,
  data = train_data,
  method = "pls",
  tuneGrid = pls_grid,
  preProcess = process_strat,
  metric = global_metric,
  trControl = ctrl
)
#predictions
pls_preds <- predict(pls_model, newdata = test_data)
# confusion matrix
pls_confm <- confusionMatrix(pls_preds, 
                             test_data$has_disease, 
                             positive = "Yes")
#show confusion matrix
pls_confm
#probabilities
pls_probs <- predict(pls_model, newdata = test_data, type = "prob")[, "Yes"]
#ROC
roc_pls <- roc(test_data$has_disease, pls_probs)
#AUC
auc(roc_pls)

#plot ROC
plot(roc_pls, 
     col = "purple",
     main = "ROC Curve - PLSDA")


# Variable Importance Scores
pls_importance <- varImp(pls_model, scale = FALSE)
plot(pls_importance, top = 10)
```


# K-Nearest Neighbors Classification (Graham)

```{r}
# fit the knn model
knn_model <- train(
  has_disease ~ .,
  data = train_data,
  method = "knn",
  metric = global_metric,
  tuneGrid = K_grid,
  trControl = ctrl,
  preProcess = process_strat
)

# get the best k via ROC
best_k <- knn_model$bestTune
#predictions
knn_preds <- predict(knn_model, newdata = test_data)

length(knn_preds)
length(test_data$has_disease)

#confusion matrix
knn_confm <- confusionMatrix(knn_preds, 
                             test_data$has_disease, 
                             positive = "Yes")
# show confusion matrix
knn_confm
#probabilities
knn_probs <- predict(knn_model, 
                     newdata = test_data,
                     type = "prob")[, "Yes"]
#roc
roc_knn <- roc(test_data$has_disease,
               knn_probs)
#auc curve
auc(roc_knn)

#plot ROC across K
ggplot(knn_model) + 
  labs(title = "KNN Performance - ROC vs K",
       x = "Number of Neighbors (K)",
       y = "ROC AUC")

#plot best curve
plot(roc_knn,
     col = "darkgreen",
     main = "ROC Curve - KNN")
```





















---
title: "Performance"
output: html_document
---

---

## title: "Model Performance Metrics" author: "Darren" output: html\_document: toc: true toc\_depth: 2

```{r libraries}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
# Load required libraries
library(tidyverse)
library(caret)
library(pROC)
library(reshape2)
```

## 1. Load Models and Test Data

```{r load}
# Load test set
processed_path <- "processed_data"
heart_bal <- readRDS(file.path(processed_path, "heart_disease_balanced.rds"))
set.seed(2025)
train_index <- createDataPartition(heart_bal$num, p=0.7, list=FALSE)
train <- heart_bal[train_index, ]
test  <- heart_bal[-train_index, ]

# Load trained models
models <- list(
  Logistic = readRDS("models/logistic_model.rds"),
  LDA = readRDS("models/lda_model.rds"),
  RandomForest = readRDS("models/random_forest_final.rds"),
  ElasticNet = readRDS("models/elasticnet_model.rds")
)
```

## 2. Predictions and Confusion Matrices

```{r metrics extraction}
results_raw <- map(models, ~ predict(.x, test))
conf_mats <- map(results_raw, ~ confusionMatrix(.x, test$num))

# Extract metrics: Accuracy, Kappa, sensitivity, specificity
metrics <- map_df(conf_mats, function(cm) {
  tibble(
    Accuracy = cm$overall['Accuracy'],
    Kappa = cm$overall['Kappa'],
    Sensitivity = cm$byClass['Sensitivity'],
    Specificity = cm$byClass['Specificity'],
    Precision = cm$byClass['Pos Pred Value'],
    Recall = cm$byClass['Sensitivity'],
    F1 = cm$byClass['F1']
  )
}, .id = 'Model')

metrics %>%
  mutate(across(-Model, round, 3)) %>%
  knitr::kable(caption = "Performance Metrics for Each Model")
```

## 3. ROC and AUC

```{r probabilities}
# Get probabilities
results_prob <- map(models, ~ predict(.x, test, type='prob'))

# Compute ROC and AUC per model (binary classification)
roc_list <- map(results_prob, function(probs) {
  roc(response = test$num,
      predictor = probs[[levels(test$num)[2]]])
})

auc_df <- tibble(
  Model = names(roc_list),
  AUC = map_dbl(roc_list, ~ auc(.x))
)

# Display AUC
auc_df %>%
  mutate(AUC = round(AUC, 3)) %>%
  knitr::kable(caption = "AUC for Each Model")

# Plot combined ROC curves
plot(roc_list[[1]], col='blue', main='ROC Curves Comparison')
for(i in 2:length(roc_list)) lines(roc_list[[i]], col = i+1)
legend('bottomright', legend = names(roc_list), col = 1:length(roc_list), lwd=2)
```

## 4. Summary and Recommendations

- The table above shows accuracy, Kappa, sensitivity (recall), specificity, precision, and F1-score for each model evaluated on the **hold-out test set**.
- AUC table and ROC plot provide discrimination power comparison.
- **Recommendation:** Based on balanced metrics (e.g., highest F1 and AUC), select the model that best balances sensitivity and specificity for your application context.


---
title: "Random_Forest"
output: pdf_document
date: "2025-06-13"
---

# Modeling 

```{r}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
library(caret)
library(here)
library(pROC)
```

## Read in datasets
```{r}
df_model <- readRDS(here("Data/Clean", "heart_disease_modeling.rds"))
df_final <- readRDS(here("Data/Clean", "heart_disease_final.rds"))

seed <- 13579
set.seed(seed)
head(df_model)
```
## Ensure positive class and balance
```{r}
# ensure positive class
df_model$has_disease <- factor(df_model$has_disease, levels = c("No", "Yes"))

# check for class balance
table(df_model$has_disease)
prop.table(table(df_model$has_disease))
```

## Train/Test Split
```{r}
train_index <- createDataPartition(df_model$has_disease, p = 0.8, list = FALSE)

train_data <- df_model[train_index, ]
test_data <- df_model[-train_index, ]

```

# Random Forest

```{r}
# train random forest model
ctrl <- trainControl(
  method = "cv", 
  number = 5, 
  classProbs = TRUE, 
  summaryFunction = twoClassSummary, 
  verboseIter = FALSE
)

rfm_model <- train(
  has_disease ~ ., 
  data = train_data, 
  method = "rf", 
  trControl = ctrl, 
  metric = "ROC", 
  importance = TRUE
)

levels(test_data$has_disease)

rfm_preds <- predict(rfm_model, test_data)
confusionMatrix(rfm_preds, test_data$has_disease, positive = "Yes")

rfm_probs <- predict(rfm_model, test_data, type = "prob")

varImp(rfm_model) %>%
  plot()
```

```{r}
# Predict on training data
train_preds <- predict(rfm_model, train_data)

# Predict on test data
test_preds <- predict(rfm_model, test_data)

```

```{r}
# Training set performance
train_cm <- confusionMatrix(train_preds, train_data$has_disease, positive = "Yes")
print(train_cm)

# Test set performance
test_cm <- confusionMatrix(test_preds, test_data$has_disease, positive = "Yes")
print(test_cm)

```

```{r}
library(pROC)

# Class probabilities
train_probs <- predict(rfm_model, train_data, type = "prob")[, "Yes"]
test_probs  <- predict(rfm_model, test_data, type = "prob")[, "Yes"]

# ROC curves
roc_train <- roc(train_data$has_disease, train_probs)
roc_test  <- roc(test_data$has_disease, test_probs)

# Print AUC
cat("Train AUC:", auc(roc_train), "\n")
cat("Test AUC:", auc(roc_test), "\n")

# Optional: plot both curves
plot(roc_train, col = "blue", main = "ROC Curves: Train vs Test")
lines(roc_test, col = "red")
legend("bottomright", legend = c("Train", "Test"), col = c("blue", "red"), lwd = 2)

```
```{r}
cat("Train AUC:", auc(roc_train), "\n")
cat("Test AUC:", auc(roc_test), "\n")

```


# Logistic Regression

```{r}
glm_model <- train(
  has_disease ~ ., 
  data = train_data,
  method = "glm", 
  family = "binomial",
  metric = "ROC",
  trControl = ctrl
)

# Class predictions
glm_preds <- predict(glm_model, test_data)

# Confusion matrix
confusionMatrix(glm_preds, test_data$has_disease, positive = "Yes")

```

```{r}
glm_probs <- predict(glm_model, test_data, type = "prob")[, "Yes"]
roc_glm <- roc(test_data$has_disease, glm_probs)

# AUC
auc(roc_glm)

# Plot
plot(roc_glm, main = "ROC Curve - Logistic Regression", col = "blue")

```


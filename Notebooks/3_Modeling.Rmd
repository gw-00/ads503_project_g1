---
title: "Modeling Notebook"
output: pdf_document
date: "2025-06-13"
---

# Modeling 

```{r}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
library(caret)
library(here)
library(pROC)
library(pls)
```

## Read and Prepare Data
```{r read_data}
# Read in modeling dataset
df_model <- readRDS(here("Data/Clean", "heart_disease_modeling.rds"))
# Ensure factor levels for binary outcome
df_model$has_disease <- factor(df_model$has_disease, levels = c("No", "Yes"))

# Set seed for reproducibility
seed <- 13579
set.seed(seed)
```

## Train/Test Split
```{r split_data}
train_index <- createDataPartition(df_model$has_disease, p = 0.8, list = FALSE)
train_data  <- df_model[train_index, ]
test_data   <- df_model[-train_index, ]
```

## Global Model Settings
```{r global_settings}
# Performance metric and resampling
global_metric <- "ROC"
ctrl <- trainControl(
  method          = "cv",
  number          = 5,
  classProbs      = TRUE,
  summaryFunction = twoClassSummary,
  verboseIter     = FALSE
)
# preprocessing strategy
process_strat <- c("center", "scale")

# Tuning grids
K_grid <- data.frame(k = seq(1, 25, by = 2))
pls_grid <- expand.grid(ncomp = 1:10)
penalty_grid <- expand.grid(
  alpha  = c(0, 1, seq(0.1, 0.9, by = 0.1)),
  lambda = 10^seq(-4, 0, length = 50)
)
```

## Initialize Results Table
```{r init_results}
results_tbl <- tibble(
  Model       = character(),
  Accuracy    = numeric(),
  Sensitivity = numeric(),
  Specificity = numeric(),
  ROC_AUC     = numeric()
)
```

### 1. Logistic Regression
```{r logistic_regression}
set.seed(seed)
glm_model <- train(
  has_disease ~ ., 
  data      = train_data,
  method    = "glm",
  family    = "binomial",
  metric    = global_metric,
  trControl = ctrl,
  preProcess = process_strat
)

glm_preds <- predict(glm_model, test_data)
glm_probs <- predict(glm_model, test_data, type = "prob")[, "Yes"]
roc_glm   <- roc(test_data$has_disease, glm_probs)
glm_cm    <- confusionMatrix(glm_preds, test_data$has_disease, positive = "Yes")

results_tbl <- results_tbl %>%
  add_row(
    Model       = "Logistic Regression",
    Accuracy    = glm_cm$overall["Accuracy"],
    Sensitivity = glm_cm$byClass["Sensitivity"],
    Specificity = glm_cm$byClass["Specificity"],
    ROC_AUC     = as.numeric(auc(roc_glm))
  )
```

### 2. Random Forest
```{r random_forest}
set.seed(seed)
rf_model <- train(
  has_disease ~ ., 
  data      = train_data,
  method    = "rf",
  metric    = global_metric,
  trControl = ctrl,
  preProcess = process_strat,
  importance = TRUE
)

rf_preds <- predict(rf_model, test_data)
rf_probs <- predict(rf_model, test_data, type = "prob")[, "Yes"]
roc_rf   <- roc(test_data$has_disease, rf_probs)
rf_cm    <- confusionMatrix(rf_preds, test_data$has_disease, positive = "Yes")

results_tbl <- results_tbl %>%
  add_row(
    Model       = "Random Forest",
    Accuracy    = rf_cm$overall["Accuracy"],
    Sensitivity = rf_cm$byClass["Sensitivity"],
    Specificity = rf_cm$byClass["Specificity"],
    ROC_AUC     = as.numeric(auc(roc_rf))
  )
```

### 3. PLS Discriminant Analysis
```{r plsda}
set.seed(seed)
pls_model <- train(
  has_disease ~ ., 
  data      = train_data,
  method    = "pls",
  tuneGrid  = pls_grid,
  preProcess = process_strat,
  metric    = global_metric,
  trControl = ctrl
)

pls_preds <- predict(pls_model, test_data)
pls_probs <- predict(pls_model, test_data, type = "prob")[, "Yes"]
roc_pls   <- roc(test_data$has_disease, pls_probs)
pls_cm    <- confusionMatrix(pls_preds, test_data$has_disease, positive = "Yes")

results_tbl <- results_tbl %>%
  add_row(
    Model       = "PLS-DA",
    Accuracy    = pls_cm$overall["Accuracy"],
    Sensitivity = pls_cm$byClass["Sensitivity"],
    Specificity = pls_cm$byClass["Specificity"],
    ROC_AUC     = as.numeric(auc(roc_pls))
  )
```

### 4. K-Nearest Neighbors
```{r knn_model}
set.seed(seed)
knn_model <- train(
  has_disease ~ ., 
  data      = train_data,
  method    = "knn",
  tuneGrid  = K_grid,
  metric    = global_metric,
  trControl = ctrl,
  preProcess = process_strat
)

knn_preds <- predict(knn_model, test_data)
knn_probs <- predict(knn_model, test_data, type = "prob")[, "Yes"]
roc_knn   <- roc(test_data$has_disease, knn_probs)
knn_cm    <- confusionMatrix(knn_preds, test_data$has_disease, positive = "Yes")

results_tbl <- results_tbl %>%
  add_row(
    Model       = "K-Nearest Neighbors",
    Accuracy    = knn_cm$overall["Accuracy"],
    Sensitivity = knn_cm$byClass["Sensitivity"],
    Specificity = knn_cm$byClass["Specificity"],
    ROC_AUC     = as.numeric(auc(roc_knn))
  )
```

### 5. Penalized Regression (Lasso, Ridge & Elastic Net)
```{r penalized_models}
set.seed(seed)
glmnet_mod <- train(
  has_disease ~ ., 
  data      = train_data,
  method    = "glmnet",
  tuneGrid  = penalty_grid,
  metric    = global_metric,
  trControl = ctrl,
  preProcess = process_strat
)

net_preds <- predict(glmnet_mod, test_data)
net_probs <- predict(glmnet_mod, test_data, type = "prob")[, "Yes"]
roc_net   <- roc(test_data$has_disease, net_probs)
net_cm    <- confusionMatrix(net_preds, test_data$has_disease, positive = "Yes")

results_tbl <- results_tbl %>%
  add_row(
    Model       = "Elastic Net (glmnet)",
    Accuracy    = net_cm$overall["Accuracy"],
    Sensitivity = net_cm$byClass["Sensitivity"],
    Specificity = net_cm$byClass["Specificity"],
    ROC_AUC     = as.numeric(auc(roc_net))
  )
```

## Results Summary and ROC Comparison
```{r results_and_plots}
# Display performance table
knitr::kable(results_tbl, digits = 3, caption = "Model Performance Comparison")

# ROC comparison plot
plot(roc_glm, col = "blue", lwd = 2, main = "ROC Curves Comparison")
lines(roc_rf,  col = "red",    lwd = 2)
lines(roc_pls, col = "purple", lwd = 2)
lines(roc_knn, col = "darkgreen", lwd = 2)
lines(roc_net, col = "orange", lwd = 2)
legend("bottomright",
       legend = results_tbl$Model,
       col    = c("blue","red","purple","darkgreen","orange"),
       lwd    = 2)
```
















